{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce301ce-5866-4f3b-9513-b1c4ad3aa63e",
   "metadata": {},
   "source": [
    "Docling RAG test based on https://github.com/docling-project/docling/blob/main/docs/examples/rag_milvus.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed9ab05-b5dd-48fe-83ae-18ee848e3bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pymilvus docling openai torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e25e20a-722f-4d57-96ac-817595ba492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA GPU is enabled: NVIDIA GeForce RTX 3060 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU or MPS is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"CUDA GPU is enabled: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"MPS GPU is enabled.\")\n",
    "else:\n",
    "    raise OSError(\n",
    "        \"No GPU or MPS device found. Please check your environment and ensure GPU or MPS support is configured.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf72082-dccf-4657-8df8-380ed91cd0fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97e5303-9393-4469-9fff-eda773aaaba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3f24af-314f-4c90-b2e7-5547bcc535f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def emb_text(text):\n",
    "    return (\n",
    "        openai_client.embeddings.create(input=text, model=\"text-embedding-3-small\")\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f00668-5de3-429d-bd60-d77bb39797ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1536\n",
      "[0.009889289736747742, -0.005578675772994757, 0.00683477520942688, -0.03805781528353691, -0.01824733428657055, -0.04121600463986397, -0.007636285852640867, 0.03225184231996536, 0.018949154764413834, 9.352207416668534e-05]\n"
     ]
    }
   ],
   "source": [
    "test_embedding = emb_text(\"This is a test\")\n",
    "embedding_dim = len(test_embedding)\n",
    "print(embedding_dim)\n",
    "print(test_embedding[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374ebb5-9bf4-4740-9e96-0e1495e69911",
   "metadata": {},
   "source": [
    "Process Data Using Docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef404071-e905-4906-9048-2f9a2a20b23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_core.transforms.chunker import HierarchicalChunker\n",
    "\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "chunker = HierarchicalChunker()\n",
    "\n",
    "# Convert the input file to Docling Document\n",
    "source = \"https://milvus.io/docs/overview.md\"\n",
    "doc = converter.convert(source).document\n",
    "\n",
    "# Perform hierarchical chunking\n",
    "texts = [chunk.text for chunk in chunker.chunk(doc)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5425f5a7-3416-4593-8934-28bf02e3998b",
   "metadata": {},
   "source": [
    "Load Data into Milvus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75386165-6a6d-4054-9d8c-3db9691fa003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymilvus import MilvusClient\n",
    "\n",
    "milvus_client = MilvusClient(uri=\"./milvus_demo.db\")\n",
    "collection_name = \"my_rag_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c86fc27-df97-4aab-9695-d83fd12eda1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if milvus_client.has_collection(collection_name):\n",
    "    milvus_client.drop_collection(collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029de9f2-901c-40b8-8675-c666ec691f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "milvus_client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    dimension=embedding_dim,\n",
    "    metric_type=\"IP\",  # Inner product distance\n",
    "    consistency_level=\"Strong\",  # Supported values are (`\"Strong\"`, `\"Session\"`, `\"Bounded\"`, `\"Eventually\"`). See https://milvus.io/docs/consistency.md#Consistency-Level for more details.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be876ab9-54ea-4aa8-9e16-7e91e9cf0713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 38/38 [00:21<00:00,  1.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'insert_count': 38, 'ids': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37], 'cost': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "data = []\n",
    "\n",
    "for i, chunk in enumerate(tqdm(texts, desc=\"Processing chunks\")):\n",
    "    embedding = emb_text(chunk)\n",
    "    data.append({\"id\": i, \"vector\": embedding, \"text\": chunk})\n",
    "\n",
    "milvus_client.insert(collection_name=collection_name, data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265f1e9-4f66-45aa-b9ec-95a214bb18ef",
   "metadata": {},
   "source": [
    "Build RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66afeafc-c631-443b-8f73-91a1bdcd695c",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = (\n",
    "    \"What are the three deployment modes of Milvus, and what are their differences?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a0ad0a2a-6535-4c61-83d9-58ab91a9ac5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_res = milvus_client.search(\n",
    "    collection_name=collection_name,\n",
    "    data=[emb_text(question)],\n",
    "    limit=3,\n",
    "    search_params={\"metric_type\": \"IP\", \"params\": {}},\n",
    "    output_fields=[\"text\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58ec5729-fa73-454d-957d-4367c7ec357b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    [\n",
      "        \"Milvus offers three deployment modes, covering a wide range of data scales\\u2014from local prototyping in Jupyter Notebooks to massive Kubernetes clusters managing tens of billions of vectors:\",\n",
      "        0.6503317356109619\n",
      "    ],\n",
      "    [\n",
      "        \"- Milvus Lite is a Python library that can be easily integrated into your applications. As a lightweight version of Milvus, it\\u2019s ideal for quick prototyping in Jupyter Notebooks or running on edge devices with limited resources. Learn more.\\n- Milvus Standalone is a single-machine server deployment, with all components bundled into a single Docker image for convenient deployment. Learn more.\\n- Milvus Distributed can be deployed on Kubernetes clusters, featuring a cloud-native architecture designed for billion-scale or even larger scenarios. This architecture ensures redundancy in critical components. Learn more.\",\n",
      "        0.634893536567688\n",
      "    ],\n",
      "    [\n",
      "        \"- What is Milvus?\\n- Unstructured Data, Embeddings, and Milvus\\n- What Makes Milvus so Fast\\uff1f\\n- What Makes Milvus so Scalable\\n- Types of Searches Supported by Milvus\\n- Comprehensive Feature Set\",\n",
      "        0.6141492128372192\n",
      "    ]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "retrieved_lines_with_distances = [\n",
    "    (res[\"entity\"][\"text\"], res[\"distance\"]) for res in search_res[0]\n",
    "]\n",
    "print(json.dumps(retrieved_lines_with_distances, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841bf071-df57-4381-88f6-a7abdf2b8607",
   "metadata": {},
   "source": [
    "Use LLM to get a RAG response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d19acc69-7a42-4a9e-9261-0e8fa3ca0e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\".join(\n",
    "    [line_with_distance[0] for line_with_distance in retrieved_lines_with_distances]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ccd1e6bd-311c-42d1-8886-e23a58b8867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "Human: You are an AI assistant. You are able to find answers to the questions from the contextual passage snippets provided.\n",
    "\"\"\"\n",
    "USER_PROMPT = f\"\"\"\n",
    "Use the following pieces of information enclosed in <context> tags to provide an answer to the question enclosed in <question> tags.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "<question>\n",
    "{question}\n",
    "</question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e17899f0-513a-46d4-adb2-1881b94d5159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milvus offers three deployment modes:\n",
      "\n",
      "1. **Milvus Lite**: This is a Python library that is ideal for quick prototyping in Jupyter Notebooks or for use on edge devices with limited resources. It is the lightweight version of Milvus and can be easily integrated into applications.\n",
      "\n",
      "2. **Milvus Standalone**: This deployment mode is a single-machine server deployment, where all the components are bundled into a single Docker image for convenient deployment. It is suitable for environments where a single machine is sufficient.\n",
      "\n",
      "3. **Milvus Distributed**: Designed for large-scale scenarios, it can be deployed on Kubernetes clusters. It features a cloud-native architecture suitable for managing billion-scale or even larger data. This mode ensures redundancy in critical components and is intended for massive deployments.\n",
      "\n",
      "Each mode is tailored to different scales and environments, from local development and prototyping to massive, production-level deployments.\n"
     ]
    }
   ],
   "source": [
    "response = openai_client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "    ],\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8f3515-a7d6-493f-b6c4-db206a182300",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
